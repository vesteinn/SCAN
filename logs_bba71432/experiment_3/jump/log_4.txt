Starting SCAN training
Namespace(bsz=1, clip=5, device='cuda', dropout=0.1, eval_interval=105000, hidden_dim=100, layers=1, log_target_probs=True, lr=0.001, model='lstm', name='../logs/experiment_3/jump/model_4.pt', seed=None, steps=100000, teacher_forcing_ratio=0.5, train='../data/SCAN/add_prim_split/tasks_train_addprim_jump.txt', use_attention=True, use_oracle=False, valid='../data/SCAN/add_prim_split/tasks_test_addprim_jump.txt', verbose=True)
----------
Loaded train dataset with 14670 entries
Loaded validation dataset with 7706 entries
Step 0 - training loss: 0.039390288293361664
Step 1000 - training loss: 14.7772798538208
Step 2000 - training loss: 8.783583641052246
Step 3000 - training loss: 6.547623634338379
Step 4000 - training loss: 4.967020034790039
Step 5000 - training loss: 4.439898490905762
Step 6000 - training loss: 4.043961524963379
Step 7000 - training loss: 4.060042858123779
Step 8000 - training loss: 3.1883673667907715
Step 9000 - training loss: 3.3817410469055176
Step 10000 - training loss: 2.6960744857788086
Step 11000 - training loss: 2.433338165283203
Step 12000 - training loss: 2.472768545150757
Step 13000 - training loss: 2.177638053894043
Step 14000 - training loss: 2.4884111881256104
Step 15000 - training loss: 2.4082653522491455
Step 16000 - training loss: 2.220829486846924
Step 17000 - training loss: 2.3606526851654053
Step 18000 - training loss: 1.7601923942565918
Step 19000 - training loss: 1.9563751220703125
Step 20000 - training loss: 1.7930035591125488
Step 21000 - training loss: 1.8335810899734497
Step 22000 - training loss: 1.5628852844238281
Step 23000 - training loss: 1.8227826356887817
Step 24000 - training loss: 1.6081849336624146
Step 25000 - training loss: 1.611696720123291
Step 26000 - training loss: 1.7666434049606323
Step 27000 - training loss: 1.7405545711517334
Step 28000 - training loss: 1.43944251537323
Step 29000 - training loss: 1.202410340309143
Step 30000 - training loss: 1.0395551919937134
Step 31000 - training loss: 1.1876473426818848
Step 32000 - training loss: 1.0377724170684814
Step 33000 - training loss: 1.2915719747543335
Step 34000 - training loss: 0.9200668931007385
Step 35000 - training loss: 1.74394690990448
Step 36000 - training loss: 1.132500171661377
Step 37000 - training loss: 0.9619415998458862
Step 38000 - training loss: 1.0017644166946411
Step 39000 - training loss: 1.2970119714736938
Step 40000 - training loss: 1.0098876953125
Step 41000 - training loss: 0.6300714015960693
Step 42000 - training loss: 0.8215153813362122
Step 43000 - training loss: 0.8708856105804443
Step 44000 - training loss: 0.9675410985946655
Step 45000 - training loss: 0.7143434286117554
Step 46000 - training loss: 1.04556143283844
Step 47000 - training loss: 0.7907513976097107
Step 48000 - training loss: 0.7688132524490356
Step 49000 - training loss: 0.9083019495010376
Step 50000 - training loss: 0.5204201936721802
Step 51000 - training loss: 0.7428878545761108
Step 52000 - training loss: 0.35163170099258423
Step 53000 - training loss: 0.6264347434043884
Step 54000 - training loss: 0.9564235806465149
Step 55000 - training loss: 0.8328920602798462
Step 56000 - training loss: 0.9742505550384521
Step 57000 - training loss: 0.3419482111930847
Step 58000 - training loss: 0.33236414194107056
Step 59000 - training loss: 0.5516247749328613
Step 60000 - training loss: 0.33895713090896606
Step 61000 - training loss: 0.959025502204895
Step 62000 - training loss: 0.25357791781425476
Step 63000 - training loss: 0.34206175804138184
Step 64000 - training loss: 0.6324962973594666
Step 65000 - training loss: 0.9430849552154541
Step 66000 - training loss: 0.289932519197464
Step 67000 - training loss: 0.34470731019973755
Step 68000 - training loss: 0.5495769381523132
Step 69000 - training loss: 0.1659265160560608
Step 70000 - training loss: 0.38469505310058594
Step 71000 - training loss: 0.4858855605125427
Step 72000 - training loss: 0.305891215801239
Step 73000 - training loss: 0.5372301936149597
